{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict if a YouTube comment is a spam\n",
    "### Import the data as pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.utils.data_utils import get_file\n",
    "import cPickle as pickle\n",
    "import bcolz\n",
    "\n",
    "\n",
    "def get_glove_dataset(dataset):\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)\n",
    "\n",
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>CONTENT_WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[huh, ,, anyway, check, out, this, you, [, tub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hey, guys, check, out, my, new, channel, and,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, for, test, i, have, to, say, murdev.com]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "\n",
       "   CLASS                                      CONTENT_WORDS  \n",
       "0      1  [huh, ,, anyway, check, out, this, you, [, tub...  \n",
       "1      1  [hey, guys, check, out, my, new, channel, and,...  \n",
       "2      1    [just, for, test, i, have, to, say, murdev.com]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_filenames = ['Youtube01-Psy.csv', 'Youtube02-KatyPerry.csv', 'Youtube03-LMFAO.csv', 'Youtube04-Eminem.csv']\n",
    "valid_filename = 'Youtube05-Shakira.csv'\n",
    "\n",
    "train_df = pd.concat([pd.read_csv('data/' + filename, encoding='utf-8-sig') for filename in train_filenames])\n",
    "\n",
    "train_df.CONTENT.head()\n",
    "\n",
    "def replace_url(phrase):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', phrase)\n",
    "    for url in urls:\n",
    "        phrase = phrase.replace(url, 'LINKHTTP')\n",
    "    return phrase\n",
    "\n",
    "def format_phrase(phrase):\n",
    "    phrase = replace_url(phrase)\n",
    "    words = re.sub(\"[^\\w]\", \" \",  phrase).split()\n",
    "    words = nltk.word_tokenize(phrase)\n",
    "    return [w.replace(\" \", \"\").lower() for w in words]\n",
    "    \n",
    "def get_unique_words(phrases):\n",
    "    words_list = phrases.sum()\n",
    "    return np.unique(np.array(words_list))\n",
    "\n",
    "def words2idxs(phrase):\n",
    "    words_count = len(wordidx) - 1\n",
    "    return [wordidx[word] if word in wordidx else words_count for word in phrase]\n",
    "\n",
    "train_df = train_df.assign(CONTENT_WORDS=train_df.CONTENT.apply(format_phrase))\n",
    "\n",
    "#unique_words = get_unique_words(train_df.CONTENT_WORDS)\n",
    "#word2idx = {v: k for k, v in enumerate(unique_words)}\n",
    "\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get words indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agnes', 'blog', 'is', 'totally', 'awesome', ':', ')', '!', '!', '!', '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_phrase('Agnes Blog is totALLy awesome :) !!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "train_df = train_df.assign(CONTENT_IDX=train_df.CONTENT_WORDS.apply(words2idxs))\n",
    "\n",
    "maxlen = train_df.CONTENT_IDX.map(len).max()\n",
    "train_content_idx = sequence.pad_sequences(train_df.CONTENT_IDX, maxlen=maxlen, value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('data/' + valid_filename, encoding='utf-8-sig')\n",
    "valid_df = valid_df.assign(CONTENT_WORDS=valid_df.CONTENT.apply(format_phrase))\n",
    "valid_df = valid_df.assign(CONTENT_IDX=valid_df.CONTENT_WORDS.apply(words2idxs))\n",
    "valid_content_idx = sequence.pad_sequences(valid_df.CONTENT_IDX, maxlen=maxlen, value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:13: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1586 samples, validate on 370 samples\n",
      "Epoch 1/3\n",
      "1586/1586 [==============================] - 2s - loss: 831.5376 - acc: 0.5076 - val_loss: 831.3093 - val_acc: 0.4703\n",
      "Epoch 2/3\n",
      "1586/1586 [==============================] - 2s - loss: 831.4612 - acc: 0.5485 - val_loss: 831.2034 - val_acc: 0.7054\n",
      "Epoch 3/3\n",
      "1586/1586 [==============================] - 2s - loss: 831.4791 - acc: 0.5448 - val_loss: 831.2057 - val_acc: 0.7459\n",
      "Train on 1586 samples, validate on 370 samples\n",
      "Epoch 1/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4795 - acc: 0.5347 - val_loss: 831.1635 - val_acc: 0.7243\n",
      "Epoch 2/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4486 - acc: 0.5492 - val_loss: 831.2000 - val_acc: 0.5270\n",
      "Epoch 3/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4771 - acc: 0.5378 - val_loss: 831.1841 - val_acc: 0.7514\n",
      "Epoch 4/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4886 - acc: 0.5359 - val_loss: 831.1649 - val_acc: 0.7162\n",
      "Epoch 5/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4499 - acc: 0.5460 - val_loss: 831.1580 - val_acc: 0.7162\n",
      "Epoch 6/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4258 - acc: 0.5460 - val_loss: 831.1534 - val_acc: 0.7270\n",
      "Epoch 7/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4181 - acc: 0.5643 - val_loss: 831.1519 - val_acc: 0.7297\n",
      "Epoch 8/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4035 - acc: 0.5618 - val_loss: 831.1608 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.3990 - acc: 0.5542 - val_loss: 831.1595 - val_acc: 0.7595\n",
      "Epoch 10/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4112 - acc: 0.5347 - val_loss: 831.1531 - val_acc: 0.7189\n",
      "Train on 1586 samples, validate on 370 samples\n",
      "Epoch 1/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.3545 - acc: 0.5883 - val_loss: 831.1379 - val_acc: 0.7649\n",
      "Epoch 2/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.3693 - acc: 0.5744 - val_loss: 831.1364 - val_acc: 0.7297\n",
      "Epoch 3/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.3264 - acc: 0.6110 - val_loss: 831.1287 - val_acc: 0.7189\n",
      "Epoch 4/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.3158 - acc: 0.6198 - val_loss: 831.1268 - val_acc: 0.7189\n",
      "Epoch 5/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.2402 - acc: 0.6576 - val_loss: 831.1129 - val_acc: 0.7676\n",
      "Epoch 6/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.2201 - acc: 0.6765 - val_loss: 831.0839 - val_acc: 0.8108\n",
      "Epoch 7/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.1777 - acc: 0.7201 - val_loss: 831.0547 - val_acc: 0.8459\n",
      "Epoch 8/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.1472 - acc: 0.7377 - val_loss: 831.0349 - val_acc: 0.8730\n",
      "Epoch 9/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.1255 - acc: 0.7509 - val_loss: 831.0084 - val_acc: 0.8730\n",
      "Epoch 10/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.0947 - acc: 0.7692 - val_loss: 830.9857 - val_acc: 0.9054\n",
      "Epoch 11/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.0750 - acc: 0.7900 - val_loss: 830.9544 - val_acc: 0.9189\n",
      "Epoch 12/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.0556 - acc: 0.7963 - val_loss: 830.9322 - val_acc: 0.9243\n",
      "Epoch 13/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.0385 - acc: 0.8115 - val_loss: 830.9145 - val_acc: 0.9324\n",
      "Epoch 14/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.0166 - acc: 0.8203 - val_loss: 830.9196 - val_acc: 0.9243\n",
      "Epoch 15/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.0030 - acc: 0.8367 - val_loss: 830.9150 - val_acc: 0.9216\n",
      "Epoch 16/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9750 - acc: 0.8405 - val_loss: 830.9050 - val_acc: 0.9216\n",
      "Epoch 17/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9879 - acc: 0.8380 - val_loss: 830.8901 - val_acc: 0.9243\n",
      "Epoch 18/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9695 - acc: 0.8361 - val_loss: 830.8768 - val_acc: 0.9297\n",
      "Epoch 19/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9699 - acc: 0.8386 - val_loss: 830.8723 - val_acc: 0.9270\n",
      "Epoch 20/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9609 - acc: 0.8373 - val_loss: 830.8673 - val_acc: 0.9054\n",
      "Epoch 21/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9458 - acc: 0.8518 - val_loss: 830.8825 - val_acc: 0.8973\n",
      "Epoch 22/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9456 - acc: 0.8512 - val_loss: 830.8687 - val_acc: 0.9108\n",
      "Epoch 23/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9520 - acc: 0.8487 - val_loss: 830.8677 - val_acc: 0.9000\n",
      "Epoch 24/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9223 - acc: 0.8556 - val_loss: 830.8622 - val_acc: 0.9000\n",
      "Epoch 25/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9208 - acc: 0.8632 - val_loss: 830.8586 - val_acc: 0.9054\n",
      "Epoch 26/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9087 - acc: 0.8752 - val_loss: 830.8568 - val_acc: 0.9027\n",
      "Epoch 27/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9393 - acc: 0.8663 - val_loss: 830.8555 - val_acc: 0.9027\n",
      "Epoch 28/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9048 - acc: 0.8701 - val_loss: 830.8545 - val_acc: 0.9081\n",
      "Epoch 29/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9059 - acc: 0.8695 - val_loss: 830.8509 - val_acc: 0.9054\n",
      "Epoch 30/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8955 - acc: 0.8695 - val_loss: 830.8448 - val_acc: 0.9135\n",
      "Epoch 31/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8965 - acc: 0.8764 - val_loss: 830.8468 - val_acc: 0.9054\n",
      "Epoch 32/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8876 - acc: 0.8821 - val_loss: 830.8460 - val_acc: 0.8838\n",
      "Epoch 33/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8817 - acc: 0.8834 - val_loss: 830.8454 - val_acc: 0.8838\n",
      "Epoch 34/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8942 - acc: 0.8764 - val_loss: 830.8444 - val_acc: 0.8811\n",
      "Epoch 35/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8993 - acc: 0.8733 - val_loss: 830.8577 - val_acc: 0.9027\n",
      "Epoch 36/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9013 - acc: 0.8733 - val_loss: 830.8527 - val_acc: 0.9054\n",
      "Epoch 37/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8832 - acc: 0.8840 - val_loss: 830.8503 - val_acc: 0.9027\n",
      "Epoch 38/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8716 - acc: 0.8802 - val_loss: 830.8503 - val_acc: 0.8811\n",
      "Epoch 39/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8707 - acc: 0.8884 - val_loss: 830.8471 - val_acc: 0.8784\n",
      "Epoch 40/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8812 - acc: 0.8808 - val_loss: 830.8597 - val_acc: 0.8838\n",
      "Train on 1586 samples, validate on 370 samples\n",
      "Epoch 1/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8733 - acc: 0.8808 - val_loss: 830.8589 - val_acc: 0.8730\n",
      "Epoch 2/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8539 - acc: 0.8903 - val_loss: 830.8591 - val_acc: 0.8730\n",
      "Epoch 3/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8513 - acc: 0.8941 - val_loss: 830.8572 - val_acc: 0.8784\n",
      "Epoch 4/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8441 - acc: 0.9023 - val_loss: 830.8564 - val_acc: 0.8784\n",
      "Epoch 5/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8488 - acc: 0.9067 - val_loss: 830.8691 - val_acc: 0.8730\n",
      "Epoch 6/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8484 - acc: 0.8953 - val_loss: 830.8579 - val_acc: 0.8973\n",
      "Epoch 7/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8384 - acc: 0.9042 - val_loss: 830.8582 - val_acc: 0.8784\n",
      "Epoch 8/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8322 - acc: 0.9073 - val_loss: 830.8572 - val_acc: 0.8811\n",
      "Epoch 9/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8342 - acc: 0.9048 - val_loss: 830.8692 - val_acc: 0.8703\n",
      "Epoch 10/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8553 - acc: 0.8922 - val_loss: 830.8490 - val_acc: 0.9081\n",
      "Epoch 11/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8383 - acc: 0.9004 - val_loss: 830.8517 - val_acc: 0.9027\n",
      "Epoch 12/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8250 - acc: 0.9136 - val_loss: 830.8569 - val_acc: 0.8946\n",
      "Epoch 13/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8340 - acc: 0.8953 - val_loss: 830.8601 - val_acc: 0.8730\n",
      "Epoch 14/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8254 - acc: 0.9111 - val_loss: 830.8610 - val_acc: 0.8757\n",
      "Epoch 15/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8232 - acc: 0.9136 - val_loss: 830.8621 - val_acc: 0.8784\n",
      "Epoch 16/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8258 - acc: 0.8991 - val_loss: 830.8560 - val_acc: 0.8757\n",
      "Epoch 17/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8159 - acc: 0.9067 - val_loss: 830.8605 - val_acc: 0.8973\n",
      "Epoch 18/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8146 - acc: 0.9067 - val_loss: 830.8679 - val_acc: 0.8784\n",
      "Epoch 19/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7995 - acc: 0.9136 - val_loss: 830.8667 - val_acc: 0.8757\n",
      "Epoch 20/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7890 - acc: 0.9256 - val_loss: 830.8687 - val_acc: 0.8784\n",
      "Epoch 21/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8026 - acc: 0.9180 - val_loss: 830.8646 - val_acc: 0.8838\n",
      "Epoch 22/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8087 - acc: 0.9117 - val_loss: 830.8708 - val_acc: 0.8784\n",
      "Epoch 23/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7938 - acc: 0.9218 - val_loss: 830.8800 - val_acc: 0.8622\n",
      "Epoch 24/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8024 - acc: 0.9142 - val_loss: 830.8758 - val_acc: 0.8730\n",
      "Epoch 25/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7864 - acc: 0.9262 - val_loss: 830.8711 - val_acc: 0.8973\n",
      "Epoch 26/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7859 - acc: 0.9212 - val_loss: 830.8659 - val_acc: 0.8973\n",
      "Epoch 27/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7861 - acc: 0.9262 - val_loss: 830.8673 - val_acc: 0.8757\n",
      "Epoch 28/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7943 - acc: 0.9187 - val_loss: 830.8752 - val_acc: 0.8730\n",
      "Epoch 29/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7937 - acc: 0.9212 - val_loss: 830.8777 - val_acc: 0.8730\n",
      "Epoch 30/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7730 - acc: 0.9313 - val_loss: 830.8778 - val_acc: 0.8703\n",
      "Epoch 31/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7970 - acc: 0.9168 - val_loss: 830.8725 - val_acc: 0.8703\n",
      "Epoch 32/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7704 - acc: 0.9243 - val_loss: 830.8705 - val_acc: 0.8757\n",
      "Epoch 33/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7897 - acc: 0.9174 - val_loss: 830.8785 - val_acc: 0.8838\n",
      "Epoch 34/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7888 - acc: 0.9193 - val_loss: 830.8875 - val_acc: 0.8649\n",
      "Epoch 35/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7810 - acc: 0.9243 - val_loss: 830.8841 - val_acc: 0.8892\n",
      "Epoch 36/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7744 - acc: 0.9275 - val_loss: 830.8760 - val_acc: 0.8676\n",
      "Epoch 37/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7748 - acc: 0.9319 - val_loss: 830.8774 - val_acc: 0.8757\n",
      "Epoch 38/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7670 - acc: 0.9319 - val_loss: 830.8832 - val_acc: 0.8730\n",
      "Epoch 39/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7844 - acc: 0.9199 - val_loss: 830.8828 - val_acc: 0.8622\n",
      "Epoch 40/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7685 - acc: 0.9294 - val_loss: 830.8844 - val_acc: 0.8838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd813ad6810>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import MaxPooling1D, Conv1D, BatchNormalization\n",
    "from keras.layers import Flatten, Dense, Embedding, Dropout, Dense, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, Adamax, RMSprop, SGD\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#embeddings_regularizer=l2(1e-4),\n",
    "\n",
    "vocab_size = len(wordidx)\n",
    "vgg_model = Sequential([\n",
    "    \n",
    "    Embedding(vocab_size, 50, input_length=maxlen, embeddings_regularizer=l2(1e-4), \n",
    "              dropout=0.2, weights=[vecs], trainable=False),\n",
    "    \n",
    "    # Conv Block 1\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    #Conv1D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Conv Block 2\n",
    "    #Conv1D(128, 3, padding='same', activation='relu'),\n",
    "    #Conv1D(128, 3, padding='same', activation='relu'),\n",
    "    #MaxPooling1D(),\n",
    "    #Dropout(0.6),\n",
    "        \n",
    "    # FC layers wiht BatchNorm\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "\n",
    "\n",
    "vgg_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "vgg_model.optimizer.lr = 10e-5\n",
    "vgg_model.fit(train_content_idx, train_df.CLASS, validation_data=(valid_content_idx, valid_df.CLASS), \n",
    "              epochs=3, batch_size=64)\n",
    "\n",
    "vgg_model.optimizer.lr = 10e-3\n",
    "vgg_model.fit(train_content_idx, train_df.CLASS, validation_data=(valid_content_idx, valid_df.CLASS), \n",
    "              epochs=10, batch_size=64)\n",
    "vgg_model.optimizer.lr = 10e-4\n",
    "vgg_model.fit(train_content_idx, train_df.CLASS, validation_data=(valid_content_idx, valid_df.CLASS), \n",
    "              epochs=40, batch_size=64)\n",
    "vgg_model.optimizer.lr = 10e-5\n",
    "vgg_model.fit(train_content_idx, train_df.CLASS, validation_data=(valid_content_idx, valid_df.CLASS), \n",
    "              epochs=40, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13810994],\n",
       "       [ 0.37778828],\n",
       "       [ 0.14262445],\n",
       "       [ 0.06725907],\n",
       "       [ 0.26699576]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_spams = ['lol love it', 'awesome video', 'i love this song', 'so many views', 'she must have so much money']\n",
    "spams = ['check my channel', 'want to have more money contact me mail', 'email me to earn a lot of money',\n",
    "         'email me to at agne@gmal.com', 'suscribe to my yt channel', 'http://salut.com']\n",
    "\n",
    "spams = [words2idxs(format_phrase(spam)) for spam in spams]\n",
    "non_spams = [words2idxs(format_phrase(spam)) for spam in non_spams]\n",
    "\n",
    "\n",
    "spams = sequence.pad_sequences(spams, maxlen=maxlen, value=-1)\n",
    "non_spams = sequence.pad_sequences(non_spams, maxlen=maxlen, value=-1)\n",
    "\n",
    "vgg_model.predict(non_spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99768353],\n",
       "       [ 0.9818902 ],\n",
       "       [ 0.96827245],\n",
       "       [ 0.98142713],\n",
       "       [ 0.97258818],\n",
       "       [ 0.49787214]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.predict(spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
