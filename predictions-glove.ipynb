{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict if a YouTube comment is a spam\n",
    "### Import the data as pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.utils.data_utils import get_file\n",
    "import cPickle as pickle\n",
    "import bcolz\n",
    "\n",
    "\n",
    "def get_glove_dataset(dataset):\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)\n",
    "\n",
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>CONTENT_WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[huh, ,, anyway, check, out, this, you, [, tub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hey, guys, check, out, my, new, channel, and,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, for, test, i, have, to, say, murdev.com]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "\n",
       "   CLASS                                      CONTENT_WORDS  \n",
       "0      1  [huh, ,, anyway, check, out, this, you, [, tub...  \n",
       "1      1  [hey, guys, check, out, my, new, channel, and,...  \n",
       "2      1    [just, for, test, i, have, to, say, murdev.com]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_filenames = ['Youtube01-Psy.csv', 'Youtube02-KatyPerry.csv', 'Youtube03-LMFAO.csv', 'Youtube04-Eminem.csv']\n",
    "valid_filename = 'Youtube05-Shakira.csv'\n",
    "\n",
    "train_df = pd.concat([pd.read_csv('data/' + filename, encoding='utf-8-sig') for filename in train_filenames])\n",
    "\n",
    "train_df.CONTENT.head()\n",
    "\n",
    "def replace_url(phrase):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', phrase)\n",
    "    for url in urls:\n",
    "        phrase = phrase.replace(url, 'LINKHTTP')\n",
    "    return phrase\n",
    "\n",
    "def format_phrase(phrase):\n",
    "    phrase = replace_url(phrase)\n",
    "    words = re.sub(\"[^\\w]\", \" \",  phrase).split()\n",
    "    words = nltk.word_tokenize(phrase)\n",
    "    return [w.replace(\" \", \"\").lower() for w in words]\n",
    "    \n",
    "def get_unique_words(phrases):\n",
    "    words_list = phrases.sum()\n",
    "    return np.unique(np.array(words_list))\n",
    "\n",
    "def words2idxs(phrase):\n",
    "    words_count = len(wordidx) - 1\n",
    "    return [wordidx[word] if word in wordidx else words_count for word in phrase]\n",
    "\n",
    "train_df = train_df.assign(CONTENT_WORDS=train_df.CONTENT.apply(format_phrase))\n",
    "\n",
    "#unique_words = get_unique_words(train_df.CONTENT_WORDS)\n",
    "#word2idx = {v: k for k, v in enumerate(unique_words)}\n",
    "\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get words indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agnes', 'blog', 'is', 'totally', 'awesome', ':', ')', '!', '!', '!', '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_phrase('Agnes Blog is totALLy awesome :) !!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "train_df = train_df.assign(CONTENT_IDX=train_df.CONTENT_WORDS.apply(words2idxs))\n",
    "\n",
    "maxlen = train_df.CONTENT_IDX.map(len).max()\n",
    "train_content_idx = sequence.pad_sequences(train_df.CONTENT_IDX, maxlen=maxlen, value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('data/' + valid_filename, encoding='utf-8-sig')\n",
    "valid_df = valid_df.assign(CONTENT_WORDS=valid_df.CONTENT.apply(format_phrase))\n",
    "valid_df = valid_df.assign(CONTENT_IDX=valid_df.CONTENT_WORDS.apply(words2idxs))\n",
    "valid_content_idx = sequence.pad_sequences(valid_df.CONTENT_IDX, maxlen=maxlen, value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [18364, 1, 5408, 2375, 66, 37, 81, 2823, 7370,...\n",
      "1    [7942, 2284, 2375, 66, 192, 50, 1629, 5, 162, ...\n",
      "2               [120, 10, 728, 41, 33, 4, 203, 399999]\n",
      "3    [285, 9174, 192, 11871, 25495, 13, 192, 1629, ...\n",
      "4            [1716, 188, 399999, 2375, 37, 66, 399999]\n",
      "Name: CONTENT_IDX, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.CONTENT_IDX.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:13: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1586 samples, validate on 370 samples\n",
      "Epoch 1/3\n",
      "1586/1586 [==============================] - 2s - loss: 831.6222 - acc: 0.5139 - val_loss: 831.2275 - val_acc: 0.7703\n",
      "Epoch 2/3\n",
      "1586/1586 [==============================] - 2s - loss: 831.5478 - acc: 0.5378 - val_loss: 831.2535 - val_acc: 0.4703\n",
      "Epoch 3/3\n",
      "1586/1586 [==============================] - 2s - loss: 831.5546 - acc: 0.5214 - val_loss: 831.1817 - val_acc: 0.7297\n",
      "Train on 1586 samples, validate on 370 samples\n",
      "Epoch 1/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.5365 - acc: 0.5322 - val_loss: 831.2062 - val_acc: 0.7811\n",
      "Epoch 2/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.5688 - acc: 0.5233 - val_loss: 831.2087 - val_acc: 0.7189\n",
      "Epoch 3/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.5107 - acc: 0.5315 - val_loss: 831.2740 - val_acc: 0.4703\n",
      "Epoch 4/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.5029 - acc: 0.5290 - val_loss: 831.2351 - val_acc: 0.4703\n",
      "Epoch 5/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.5192 - acc: 0.5328 - val_loss: 831.2043 - val_acc: 0.4703\n",
      "Epoch 6/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4819 - acc: 0.5492 - val_loss: 831.1711 - val_acc: 0.7486\n",
      "Epoch 7/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4728 - acc: 0.5422 - val_loss: 831.1612 - val_acc: 0.7703\n",
      "Epoch 8/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4703 - acc: 0.5359 - val_loss: 831.1636 - val_acc: 0.7703\n",
      "Epoch 9/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4106 - acc: 0.5763 - val_loss: 831.1827 - val_acc: 0.7865\n",
      "Epoch 10/10\n",
      "1586/1586 [==============================] - 2s - loss: 831.4116 - acc: 0.5675 - val_loss: 831.1921 - val_acc: 0.7649\n",
      "Train on 1586 samples, validate on 370 samples\n",
      "Epoch 1/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.3860 - acc: 0.5782 - val_loss: 831.1928 - val_acc: 0.7703\n",
      "Epoch 2/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.3477 - acc: 0.5750 - val_loss: 831.1863 - val_acc: 0.8081\n",
      "Epoch 3/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.2966 - acc: 0.6217 - val_loss: 831.1819 - val_acc: 0.7054\n",
      "Epoch 4/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.3222 - acc: 0.6242 - val_loss: 831.1496 - val_acc: 0.8514\n",
      "Epoch 5/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.2738 - acc: 0.6526 - val_loss: 831.1279 - val_acc: 0.8405\n",
      "Epoch 6/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.2304 - acc: 0.6929 - val_loss: 831.0863 - val_acc: 0.8946\n",
      "Epoch 7/40\n",
      "1586/1586 [==============================] - 2s - loss: 831.2034 - acc: 0.7100 - val_loss: 831.0368 - val_acc: 0.9135\n",
      "Epoch 8/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.1472 - acc: 0.7421 - val_loss: 831.0067 - val_acc: 0.8892\n",
      "Epoch 9/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.1331 - acc: 0.7711 - val_loss: 830.9781 - val_acc: 0.9324\n",
      "Epoch 10/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.1133 - acc: 0.7749 - val_loss: 830.9600 - val_acc: 0.9216\n",
      "Epoch 11/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.0877 - acc: 0.7749 - val_loss: 830.9478 - val_acc: 0.9324\n",
      "Epoch 12/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.0799 - acc: 0.7787 - val_loss: 830.9391 - val_acc: 0.9297\n",
      "Epoch 13/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.0594 - acc: 0.7907 - val_loss: 830.9236 - val_acc: 0.9270\n",
      "Epoch 14/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.0588 - acc: 0.7970 - val_loss: 830.9155 - val_acc: 0.9351\n",
      "Epoch 15/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.0315 - acc: 0.8108 - val_loss: 830.9071 - val_acc: 0.9351\n",
      "Epoch 16/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.0305 - acc: 0.8014 - val_loss: 830.9003 - val_acc: 0.9405\n",
      "Epoch 17/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.0351 - acc: 0.8235 - val_loss: 830.8894 - val_acc: 0.9324\n",
      "Epoch 18/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9938 - acc: 0.8342 - val_loss: 830.8783 - val_acc: 0.9324\n",
      "Epoch 19/40\n",
      "1586/1586 [==============================] - 1s - loss: 831.0049 - acc: 0.8197 - val_loss: 830.8760 - val_acc: 0.9351\n",
      "Epoch 20/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9763 - acc: 0.8361 - val_loss: 830.8698 - val_acc: 0.9351\n",
      "Epoch 21/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9693 - acc: 0.8342 - val_loss: 830.8659 - val_acc: 0.9324\n",
      "Epoch 22/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9742 - acc: 0.8499 - val_loss: 830.8641 - val_acc: 0.9324\n",
      "Epoch 23/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9687 - acc: 0.8443 - val_loss: 830.8618 - val_acc: 0.9162\n",
      "Epoch 24/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9434 - acc: 0.8518 - val_loss: 830.8584 - val_acc: 0.9189\n",
      "Epoch 25/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9462 - acc: 0.8487 - val_loss: 830.8558 - val_acc: 0.9216\n",
      "Epoch 26/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9342 - acc: 0.8581 - val_loss: 830.8571 - val_acc: 0.9189\n",
      "Epoch 27/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9470 - acc: 0.8581 - val_loss: 830.8696 - val_acc: 0.8865\n",
      "Epoch 28/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9160 - acc: 0.8625 - val_loss: 830.8719 - val_acc: 0.8838\n",
      "Epoch 29/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9076 - acc: 0.8676 - val_loss: 830.8534 - val_acc: 0.9162\n",
      "Epoch 30/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9292 - acc: 0.8638 - val_loss: 830.8505 - val_acc: 0.9135\n",
      "Epoch 31/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.9405 - acc: 0.8449 - val_loss: 830.8524 - val_acc: 0.9108\n",
      "Epoch 32/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9102 - acc: 0.8651 - val_loss: 830.8528 - val_acc: 0.9081\n",
      "Epoch 33/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9148 - acc: 0.8657 - val_loss: 830.8599 - val_acc: 0.8838\n",
      "Epoch 34/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9076 - acc: 0.8695 - val_loss: 830.8561 - val_acc: 0.9027\n",
      "Epoch 35/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8953 - acc: 0.8714 - val_loss: 830.8581 - val_acc: 0.9027\n",
      "Epoch 36/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9047 - acc: 0.8802 - val_loss: 830.8578 - val_acc: 0.8919\n",
      "Epoch 37/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.9163 - acc: 0.8689 - val_loss: 830.8529 - val_acc: 0.8838\n",
      "Epoch 38/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8921 - acc: 0.8625 - val_loss: 830.8535 - val_acc: 0.8865\n",
      "Epoch 39/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8861 - acc: 0.8846 - val_loss: 830.8603 - val_acc: 0.8838\n",
      "Epoch 40/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8892 - acc: 0.8789 - val_loss: 830.8609 - val_acc: 0.8811\n",
      "Train on 1586 samples, validate on 370 samples\n",
      "Epoch 1/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8776 - acc: 0.8865 - val_loss: 830.8496 - val_acc: 0.9027\n",
      "Epoch 2/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8717 - acc: 0.8916 - val_loss: 830.8529 - val_acc: 0.8838\n",
      "Epoch 3/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8689 - acc: 0.8815 - val_loss: 830.8577 - val_acc: 0.8784\n",
      "Epoch 4/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8655 - acc: 0.8846 - val_loss: 830.8539 - val_acc: 0.8892\n",
      "Epoch 5/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8677 - acc: 0.8834 - val_loss: 830.8445 - val_acc: 0.9189\n",
      "Epoch 6/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8601 - acc: 0.8878 - val_loss: 830.8508 - val_acc: 0.8865\n",
      "Epoch 7/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8653 - acc: 0.8928 - val_loss: 830.8445 - val_acc: 0.8973\n",
      "Epoch 8/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8479 - acc: 0.8878 - val_loss: 830.8428 - val_acc: 0.9216\n",
      "Epoch 9/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8523 - acc: 0.8890 - val_loss: 830.8542 - val_acc: 0.8892\n",
      "Epoch 10/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8473 - acc: 0.8991 - val_loss: 830.8550 - val_acc: 0.8892\n",
      "Epoch 11/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8426 - acc: 0.8966 - val_loss: 830.8548 - val_acc: 0.8811\n",
      "Epoch 12/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8306 - acc: 0.8934 - val_loss: 830.8579 - val_acc: 0.8892\n",
      "Epoch 13/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8376 - acc: 0.9067 - val_loss: 830.8516 - val_acc: 0.8892\n",
      "Epoch 14/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8316 - acc: 0.9010 - val_loss: 830.8496 - val_acc: 0.8946\n",
      "Epoch 15/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8396 - acc: 0.8985 - val_loss: 830.8576 - val_acc: 0.8757\n",
      "Epoch 16/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8295 - acc: 0.9035 - val_loss: 830.8498 - val_acc: 0.8946\n",
      "Epoch 17/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8309 - acc: 0.9029 - val_loss: 830.8535 - val_acc: 0.8811\n",
      "Epoch 18/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8383 - acc: 0.9061 - val_loss: 830.8564 - val_acc: 0.8784\n",
      "Epoch 19/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8335 - acc: 0.9004 - val_loss: 830.8575 - val_acc: 0.8784\n",
      "Epoch 20/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8148 - acc: 0.9042 - val_loss: 830.8556 - val_acc: 0.8865\n",
      "Epoch 21/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.8288 - acc: 0.9079 - val_loss: 830.8554 - val_acc: 0.8892\n",
      "Epoch 22/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8141 - acc: 0.9124 - val_loss: 830.8551 - val_acc: 0.8865\n",
      "Epoch 23/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8225 - acc: 0.9117 - val_loss: 830.8537 - val_acc: 0.8892\n",
      "Epoch 24/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8101 - acc: 0.9079 - val_loss: 830.8610 - val_acc: 0.9000\n",
      "Epoch 25/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8203 - acc: 0.9023 - val_loss: 830.8599 - val_acc: 0.8811\n",
      "Epoch 26/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8093 - acc: 0.9067 - val_loss: 830.8628 - val_acc: 0.8973\n",
      "Epoch 27/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8330 - acc: 0.9042 - val_loss: 830.8567 - val_acc: 0.9054\n",
      "Epoch 28/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8056 - acc: 0.9168 - val_loss: 830.8635 - val_acc: 0.8703\n",
      "Epoch 29/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.7996 - acc: 0.9168 - val_loss: 830.8613 - val_acc: 0.8784\n",
      "Epoch 30/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.7916 - acc: 0.9218 - val_loss: 830.8631 - val_acc: 0.8784\n",
      "Epoch 31/40\n",
      "1586/1586 [==============================] - 2s - loss: 830.7845 - acc: 0.9237 - val_loss: 830.8625 - val_acc: 0.8730\n",
      "Epoch 32/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8114 - acc: 0.9067 - val_loss: 830.8609 - val_acc: 0.8757\n",
      "Epoch 33/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.8003 - acc: 0.9054 - val_loss: 830.8572 - val_acc: 0.8784\n",
      "Epoch 34/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.7807 - acc: 0.9149 - val_loss: 830.8628 - val_acc: 0.8730\n",
      "Epoch 35/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.7988 - acc: 0.9130 - val_loss: 830.8577 - val_acc: 0.8784\n",
      "Epoch 36/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.7962 - acc: 0.9187 - val_loss: 830.8676 - val_acc: 0.8703\n",
      "Epoch 37/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.7906 - acc: 0.9168 - val_loss: 830.8647 - val_acc: 0.8784\n",
      "Epoch 38/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.7858 - acc: 0.9218 - val_loss: 830.8566 - val_acc: 0.8784\n",
      "Epoch 39/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.7824 - acc: 0.9250 - val_loss: 830.8731 - val_acc: 0.8676\n",
      "Epoch 40/40\n",
      "1586/1586 [==============================] - 1s - loss: 830.7728 - acc: 0.9250 - val_loss: 830.8521 - val_acc: 0.8865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff6692ec850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import MaxPooling1D, Conv1D, BatchNormalization\n",
    "from keras.layers import Flatten, Dense, Embedding, Dropout, Dense, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, Adamax, RMSprop, SGD\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#embeddings_regularizer=l2(1e-4),\n",
    "\n",
    "vocab_size = len(wordidx)\n",
    "vgg_model = Sequential([\n",
    "    \n",
    "    Embedding(vocab_size, 50, input_length=maxlen, embeddings_regularizer=l2(1e-4), \n",
    "              dropout=0.2, weights=[vecs], trainable=False),\n",
    "    \n",
    "    # Conv Block 1\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    #Conv1D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Conv Block 2\n",
    "    #Conv1D(128, 3, padding='same', activation='relu'),\n",
    "    #Conv1D(128, 3, padding='same', activation='relu'),\n",
    "    #MaxPooling1D(),\n",
    "    #Dropout(0.6),\n",
    "        \n",
    "    # FC layers wiht BatchNorm\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "\n",
    "\n",
    "vgg_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "vgg_model.optimizer.lr = 10e-5\n",
    "vgg_model.fit(train_content_idx, train_df.CLASS, validation_data=(valid_content_idx, valid_df.CLASS), \n",
    "              epochs=3, batch_size=64)\n",
    "\n",
    "vgg_model.optimizer.lr = 10e-3\n",
    "vgg_model.fit(train_content_idx, train_df.CLASS, validation_data=(valid_content_idx, valid_df.CLASS), \n",
    "              epochs=10, batch_size=64)\n",
    "vgg_model.optimizer.lr = 10e-4\n",
    "vgg_model.fit(train_content_idx, train_df.CLASS, validation_data=(valid_content_idx, valid_df.CLASS), \n",
    "              epochs=40, batch_size=64)\n",
    "vgg_model.optimizer.lr = 10e-5\n",
    "vgg_model.fit(train_content_idx, train_df.CLASS, validation_data=(valid_content_idx, valid_df.CLASS), \n",
    "              epochs=40, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03341711],\n",
       "       [ 0.40052167],\n",
       "       [ 0.0507709 ],\n",
       "       [ 0.39072457],\n",
       "       [ 0.47617087]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_spams = ['lol love it', 'awesome video', 'i love this song', 'so many views', 'she must have so much money']\n",
    "spams = ['check my channel', 'want to have more money contact me mail', 'email me to earn a lot of money',\n",
    "         'email me to at agne@gmal.com', 'suscribe to my yt channel', 'http://salut.com']\n",
    "\n",
    "spams = [words2idxs(format_phrase(spam)) for spam in spams]\n",
    "non_spams = [words2idxs(format_phrase(spam)) for spam in non_spams]\n",
    "\n",
    "\n",
    "spams = sequence.pad_sequences(spams, maxlen=maxlen, value=-1)\n",
    "non_spams = sequence.pad_sequences(non_spams, maxlen=maxlen, value=-1)\n",
    "\n",
    "vgg_model.predict(non_spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98178357],\n",
       "       [ 0.9813689 ],\n",
       "       [ 0.98000342],\n",
       "       [ 0.9867323 ],\n",
       "       [ 0.72222692],\n",
       "       [ 0.50569528]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.predict(spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
